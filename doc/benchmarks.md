# Description of benchmarks

The question level evaluation results is described under **eval results**. Some evals have many version (MBPP) since subsequent authors found it better to apply filtering, so the number of question needs to be matched with the particular version. There are also some inconsistencies due to choices made by the evaluation harness builder, depending on which subset or processing they chose to use.

## CodeGen Benchmarks

## CRUXEval
- **Size**: 800 problems × 2 tasks (input prediction, output prediction)
- **Description**: Code reasoning benchmark with 800 short Python functions. CRUXEval-I: given function + output, predict the input. CRUXEval-O: given function + input, predict the output.
- **Paper**: Gu et al., "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution" ([arXiv:2401.03065](https://arxiv.org/abs/2401.03065)), 2024
- **Questions**: [GitHub](https://github.com/facebookresearch/cruxeval) · [Leaderboard](https://crux-eval.github.io/)
- **Eval results**: [on github](https://github.com/facebookresearch/cruxeval/tree/main/samples)

### DS-1000
- **Size**: 1000 problems
- **Description**: Data science code generation spanning 7 Python libraries (NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, Matplotlib). Sourced from StackOverflow with perturbations to defend against memorization.
- **Paper**: Lai et al., "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation" ([arXiv:2211.11501](https://arxiv.org/abs/2211.11501)), ICML 2023
- **Data**: [GitHub](https://github.com/xlang-ai/DS-1000) · [HuggingFace](https://huggingface.co/datasets/xlangai/DS-1000) · [Project page](https://ds1000-code-gen.github.io/)
- **Eval results**: from the authors


### HumanEval / plus
- **Size**: 164 problems
- **Description**: Hand-written Python programming problems with function signature + docstring. Model generates the body; evaluated via unit tests (pass@k).
- **Paper**: Chen et al., "Evaluating Large Language Models Trained on Code" ([arXiv:2107.03374](https://arxiv.org/abs/2107.03374)), 2021.
- **Ppaper**: Liu et al., "Is Your Code Generated by ChatGPT Really Correct?" ([arXiv:2305.01210](https://arxiv.org/abs/2305.01210)), 2023
- **Data**: [original](https://github.com/openai/human-eval) · [EvalPlus](https://github.com/evalplus/evalplus) [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html)
- **Eval results**:  from the authors

### LiveCodeBench (lcb_codegen_v5, v6, v6_080124)
- **Size**: 880 (v5) / 1055 (v6) / 454 (v6 post-Aug 2024)
- **Description**: Continuously updated competitive programming benchmark from LeetCode, AtCoder, Codeforces. Problems tagged with release dates to measure contamination. Also covers self-repair, code execution, and test output prediction.
- **Paper**: Jain et al., "LiveCodeBench: Holistic and Contamination Free Evaluation of LLMs for Code" ([arXiv:2403.07974](https://arxiv.org/abs/2403.07974)), 2024
- **Data**: [GitHub](https://github.com/LiveCodeBench/LiveCodeBench) · [HuggingFace](https://huggingface.co/datasets/livecodebench/code_generation_lite) · [Leaderboard](https://livecodebench.github.io/)
- **Eval results**:  [LiveCodeBench submissions](https://github.com/LiveCodeBench/submissions)

### MBPP / MBPP+
- **Size**: 378 problems (sanitized subset)
- **Description**: Crowd-sourced basic Python programming problems solvable by entry-level programmers. MBPP+ adds augmented test cases via EvalPlus.
- **Paper (MBPP)**: Austin et al., "Program Synthesis with Large Language Models" ([arXiv:2108.07732](https://arxiv.org/abs/2108.07732)), 2021
- **Paper (MBPP+)**: Liu et al. ([arXiv:2305.01210](https://arxiv.org/abs/2305.01210)), 2023
- **Data**: [HuggingFace](https://huggingface.co/datasets/google-research-datasets/mbpp) · [Leaderboard](https://evalplus.github.io/leaderboard.html)
- **Eval results**:  from the authors

### SAFIM
- **Size**: 17,410 examples
- **Description**: Syntax-aware fill-in-the-middle benchmark across multiple languages (Python, Java, C++, C#). Three subtasks: algorithmic block completion, control-flow expression completion, API function call completion. Post-April 2022 to minimize contamination.
- **Paper**: Gong et al., "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks" ([arXiv:2403.04814](https://arxiv.org/abs/2403.04814)), ICML 2024
- **Data**: [GitHub](https://github.com/gonglinyuan/safim) · [HuggingFace](https://huggingface.co/datasets/gonglinyuan/safim) · [Leaderboard](https://safimbenchmark.com/)
- **Eval results**:  from the authors
---

## Agentic Benchmarks

### SWE-bench (test, lite, verified, bash-only, multimodal)
- **Size**: 2294 (full) / 300 (lite) / 500 (verified) / 500 (bash-only) / 510 (multimodal)
- **Description**: Real GitHub issues from popular repositories. Given a codebase + issue description, resolve the issue. Variants: Lite (self-contained subset), Verified (human-confirmed solvable), Bash-only (CLI-only agents), Multimodal (JavaScript/visual software with image-based issues).
- **Paper (original)**: Jimenez et al., "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?" ([arXiv:2310.06770](https://arxiv.org/abs/2310.06770)), ICLR 2024
- **Paper (multimodal)**: Yang et al., "SWE-bench Multimodal" ([arXiv:2410.03859](https://arxiv.org/abs/2410.03859)), ICLR 2025
- **Data**: [GitHub](https://github.com/SWE-bench/SWE-bench) · [HuggingFace](https://huggingface.co/datasets/princeton-nlp/SWE-bench) · [Leaderboard](https://www.swebench.com/)
- **Eval results**:  [experiments](https://github.com/swe-bench/experiments)

### Terminal-Bench (1.0, 2.0)
- **Size**: 80 (v1.0) / 89 (v2.0)
- **Description**: Hard, realistic tasks in terminal environments (compiling kernels, training ML models, configuring servers, etc.). Each task has a Docker environment, human-written solution, and comprehensive tests.
- **Paper**: Merrill, Shaw, Carlini et al., "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in CLIs" ([arXiv:2601.11868](https://arxiv.org/abs/2601.11868)), 2026
- **Data**: [GitHub](https://github.com/laude-institute/terminal-bench) · [Leaderboard](https://www.tbench.ai/)
- **Eval results**:  [terminal-bench-core@0.1.1](https://github.com/laude-institute/terminal-bench-leaderboard/tree/main/results/terminal-bench-core%400.1.1), [terminal-bench-2-leaderboard](https://huggingface.co/datasets/alexgshaw/terminal-bench-2-leaderboard/tree/main/submissions/terminal-bench/2.0)
---

## Knowledge & Reasoning Benchmarks

### AGIEval (English subset, `agi_english`)
- **Size**: 2546 questions
- **Description**: English subset of AGIEval — questions from official admission/qualification exams (SAT, LSAT, Gaokao, civil service exams) testing human-level cognition and problem-solving.
- **Paper**: Zhong et al., "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models" ([arXiv:2304.06364](https://arxiv.org/abs/2304.06364)), 2023
- **Data**: [GitHub](https://github.com/ruixiangcui/AGIEval)
- **Eval results**: ran by collaborators

### ARC Challenge
- **Size**: 1165 questions
- **Description**: Grade-school science multiple-choice questions that are difficult for retrieval-based and co-occurrence methods. The "Challenge" set filters for questions most models get wrong.
- **Paper**: Clark et al., "Think you have Solved Question Answering? Try ARC" ([arXiv:1803.05457](https://arxiv.org/abs/1803.05457)), 2018
- **Data**: [HuggingFace](https://huggingface.co/datasets/allenai/ai2_arc)
- **Eval results**: ran by collaborators

### GSM8K
- **Size**: 1319 (test set)
- **Description**: Grade-school math word problems requiring 2–8 steps of basic arithmetic.
- **Paper**: Cobbe et al., "Training Verifiers to Solve Math Word Problems" ([arXiv:2110.14168](https://arxiv.org/abs/2110.14168)), 2021
- **Data**: [GitHub](https://github.com/openai/grade-school-math) · [HuggingFace](https://huggingface.co/datasets/openai/gsm8k)
- **Eval results**: ran by collaborators

### HellaSwag
- **Size**: 10,042 questions
- **Description**: Commonsense NLI — choose the most plausible continuation of a scenario. Adversarially generated to be easy for humans (~95%) but hard for models.
- **Paper**: Zellers et al., "HellaSwag: Can a Machine Really Finish Your Sentence?" ([arXiv:1905.07830](https://arxiv.org/abs/1905.07830)), ACL 2019
- **Data**: [GitHub](https://github.com/rowanz/hellaswag) · [HuggingFace](https://huggingface.co/datasets/Rowan/hellaswag)
- **Eval results**: ran by collaborators

### MMLU
- **Size**: 14,042 questions
- **Description**: Multiple-choice questions across 57 subjects (STEM, humanities, social sciences, law, medicine). The standard general-knowledge LLM benchmark.
- **Paper**: Hendrycks et al., "Measuring Massive Multitask Language Understanding" ([arXiv:2009.03300](https://arxiv.org/abs/2009.03300)), ICLR 2021
- **Data**: [GitHub](https://github.com/hendrycks/test) · [HuggingFace](https://huggingface.co/datasets/cais/mmlu)
- **Eval results**: ran by collaborators

### Natural Questions (`nq`)
- **Size**: 3610 (eval subset used)
- **Description**: Open-domain QA from real Google search queries with Wikipedia-derived answers.
- **Paper**: Kwiatkowski et al., "Natural Questions: a Benchmark for Question Answering Research" ([arXiv:1906.02900](https://arxiv.org/abs/1906.02900)), TACL 2019
- **Data**: [HuggingFace](https://huggingface.co/datasets/google-research-datasets/natural_questions)
- **Eval results**: ran by collaborators

### PIQA
- **Size**: 1838 questions
- **Description**: Physical intuition QA — given a goal, choose the more sensible physical-world solution from two options.
- **Paper**: Bisk et al., "PIQA: Reasoning about Physical Intuition in Natural Language" ([arXiv:1911.11641](https://arxiv.org/abs/1911.11641)), AAAI 2020
- **Data**: [HuggingFace](https://huggingface.co/datasets/ybisk/piqa)
- **Eval results**: ran by collaborators

### Social IQA (`siqa`)
- **Size**: 1954 questions
- **Description**: Commonsense reasoning about social situations — emotional reactions, motivations, and likely next actions.
- **Paper**: Sap et al., "Social IQa: Commonsense Reasoning about Social Interactions" ([arXiv:1904.09728](https://arxiv.org/abs/1904.09728)), EMNLP 2019
- **Data**: [HuggingFace](https://huggingface.co/datasets/allenai/social_i_qa)
- **Eval results**: ran by collaborators

### TriviaQA (`tqa`)
- **Size**: 11,313 questions
- **Description**: Open-domain QA by trivia enthusiasts, with evidence documents from Wikipedia and the web.
- **Paper**: Joshi et al., "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension" ([arXiv:1705.03551](https://arxiv.org/abs/1705.03551)), ACL 2017
- **Data**: [GitHub](https://github.com/mandarjoshi90/triviaqa) · [HuggingFace](https://huggingface.co/datasets/trivia_qa)
- **Eval results**: ran by collaborators

### MATH (`math_cot`)
- **Size**: 5,000 questions
- **Description**: Problems from mathematics competitions, including the AMC 10, AMC 12, AIME, and more.
- **Paper**: Hendrycks et al., "Measuring Mathematical Problem Solving With the MATH Dataset" ([arXiv:2103.03874](https://arxiv.org/pdf/2103.03874)), NeurIPS 2021
- **Data**: [GitHub](https://github.com/hendrycks/math) · [HuggingFace](https://huggingface.co/datasets/EleutherAI/hendrycks_math)
- **Eval results**: ran by collaborators

### MATH-500 (`math500_cot`)
- **Size**: 500 questions
- **Description**: Subset of 500 problems from the MATH benchmark that OpenAI created in their Let's Verify Step by Step paper.
- **Paper**: Lightman et al., "Let's Verify Step by Step" ([arXiv:2305.20050](https://arxiv.org/pdf/2305.20050)), 2023
- **Data**: [GitHub](https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits) · [HuggingFace](https://huggingface.co/datasets/HuggingFaceH4/MATH-500)
- **Eval results**: ran by collaborators

### GPQA (`gpqa_cot`)
- **Size**: 448 questions
- **Description**: Multiple-choice questions written by domain experts in biology, physics, and chemistry.
- **Paper**: Rein et al., "GPQA: A Graduate-Level Google-Proof Q&A Benchmark" ([arXiv:2311.12022](https://arxiv.org/pdf/2311.12022)), 2023
- **Data**: [GitHub](https://github.com/idavidrein/gpqa) · [HuggingFace](https://huggingface.co/datasets/Idavidrein/gpqa)
- **Eval results**: ran by collaborators
  
